{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dab3a9",
   "metadata": {},
   "source": [
    "# TITANIC - DATASET CSV for Real World Problem - Practice - PANDAS (IMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df66d27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "________________________________________________________________________________\n",
      "     PassengerId  Survived  Pclass                                      Name  \\\n",
      "886          887         0       2                     Montvila, Rev. Juozas   \n",
      "887          888         1       1              Graham, Miss. Margaret Edith   \n",
      "888          889         0       3  Johnston, Miss. Catherine Helen \"Carrie\"   \n",
      "889          890         1       1                     Behr, Mr. Karl Howell   \n",
      "890          891         0       3                       Dooley, Mr. Patrick   \n",
      "\n",
      "        Sex   Age  SibSp  Parch      Ticket   Fare Cabin Embarked  \n",
      "886    male  27.0      0      0      211536  13.00   NaN        S  \n",
      "887  female  19.0      0      0      112053  30.00   B42        S  \n",
      "888  female   NaN      1      2  W./C. 6607  23.45   NaN        S  \n",
      "889    male  26.0      0      0      111369  30.00  C148        C  \n",
      "890    male  32.0      0      0      370376   7.75   NaN        Q  \n",
      "(891, 12)\n",
      "________________________________________________________________________________\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "________________________________________________________________________________\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "________________________________________________________________________________\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "PassengerId     0.000000\n",
      "Survived        0.000000\n",
      "Pclass          0.000000\n",
      "Name            0.000000\n",
      "Sex             0.000000\n",
      "Age            19.865320\n",
      "SibSp           0.000000\n",
      "Parch           0.000000\n",
      "Ticket          0.000000\n",
      "Fare            0.000000\n",
      "Cabin          77.104377\n",
      "Embarked        0.224467\n",
      "dtype: float64\n",
      "________________________________________________________________________________\n",
      "0\n",
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "..           ...       ...     ...   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                             Allen, Mr. William Henry    male  35.0      0   \n",
      "..                                                 ...     ...   ...    ...   \n",
      "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
      "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
      "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
      "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
      "\n",
      "     Parch            Ticket     Fare Cabin Embarked  \n",
      "0        0         A/5 21171   7.2500   NaN        S  \n",
      "1        0          PC 17599  71.2833   C85        C  \n",
      "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3        0            113803  53.1000  C123        S  \n",
      "4        0            373450   8.0500   NaN        S  \n",
      "..     ...               ...      ...   ...      ...  \n",
      "886      0            211536  13.0000   NaN        S  \n",
      "887      0            112053  30.0000   B42        S  \n",
      "888      2        W./C. 6607  23.4500   NaN        S  \n",
      "889      0            111369  30.0000  C148        C  \n",
      "890      0            370376   7.7500   NaN        Q  \n",
      "\n",
      "[891 rows x 12 columns]\n",
      "________________________________________________________________________________\n",
      "['male' 'female']\n",
      "['S' 'C' 'Q' nan]\n",
      "[3 1 2]\n",
      "________________________________________________________________________________\n",
      "mean:- 29.69911764705882\n",
      "median:- 14.4542\n",
      "Number of outliers: 439\n",
      "     PassengerId  Survived  Pclass  \\\n",
      "1              2         1       1   \n",
      "3              4         1       1   \n",
      "6              7         0       1   \n",
      "7              8         0       3   \n",
      "9             10         1       2   \n",
      "..           ...       ...     ...   \n",
      "880          881         1       2   \n",
      "885          886         0       3   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "6                              McCarthy, Mr. Timothy J    male  54.0      0   \n",
      "7                       Palsson, Master. Gosta Leonard    male   2.0      3   \n",
      "9                  Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
      "..                                                 ...     ...   ...    ...   \n",
      "880       Shelley, Mrs. William (Imanita Parrish Hall)  female  25.0      0   \n",
      "885               Rice, Mrs. William (Margaret Norton)  female  39.0      0   \n",
      "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
      "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
      "\n",
      "     Parch      Ticket     Fare Cabin Embarked  \n",
      "1        0    PC 17599  71.2833   C85        C  \n",
      "3        0      113803  53.1000  C123        S  \n",
      "6        0       17463  51.8625   E46        S  \n",
      "7        1      349909  21.0750   NaN        S  \n",
      "9        0      237736  30.0708   NaN        C  \n",
      "..     ...         ...      ...   ...      ...  \n",
      "880      1      230433  26.0000   NaN        S  \n",
      "885      5      382652  29.1250   NaN        Q  \n",
      "887      0      112053  30.0000   B42        S  \n",
      "888      2  W./C. 6607  23.4500   NaN        S  \n",
      "889      0      111369  30.0000  C148        C  \n",
      "\n",
      "[439 rows x 12 columns]\n",
      "________________________________________________________________________________\n",
      "             PassengerId  Survived    Pclass       Age     SibSp     Parch  \\\n",
      "PassengerId     1.000000 -0.005007 -0.035144  0.036847 -0.057527 -0.001652   \n",
      "Survived       -0.005007  1.000000 -0.338481 -0.077221 -0.035322  0.081629   \n",
      "Pclass         -0.035144 -0.338481  1.000000 -0.369226  0.083081  0.018443   \n",
      "Age             0.036847 -0.077221 -0.369226  1.000000 -0.308247 -0.189119   \n",
      "SibSp          -0.057527 -0.035322  0.083081 -0.308247  1.000000  0.414838   \n",
      "Parch          -0.001652  0.081629  0.018443 -0.189119  0.414838  1.000000   \n",
      "Fare            0.012658  0.257307 -0.549500  0.096067  0.159651  0.216225   \n",
      "\n",
      "                 Fare  \n",
      "PassengerId  0.012658  \n",
      "Survived     0.257307  \n",
      "Pclass      -0.549500  \n",
      "Age          0.096067  \n",
      "SibSp        0.159651  \n",
      "Parch        0.216225  \n",
      "Fare         1.000000  \n",
      "PassengerId   -0.005007\n",
      "Survived       1.000000\n",
      "Pclass        -0.338481\n",
      "Age           -0.077221\n",
      "SibSp         -0.035322\n",
      "Parch          0.081629\n",
      "Fare           0.257307\n",
      "Name: Survived, dtype: float64\n",
      "________________________________________________________________________________\n",
      "Survived\n",
      "0    549\n",
      "1    342\n",
      "Name: count, dtype: int64\n",
      "________________________________________________________________________________\n",
      "The survival rate is :- 38.38383838383838\n",
      "________________________________________________________________________________\n",
      "The average age of passengers :- 29.7\n",
      "________________________________________________________________________________\n",
      "Sex\n",
      "male      577\n",
      "female    314\n",
      "Name: count, dtype: int64\n",
      "________________________________________________________________________________\n",
      "The 3 has the most Passengers in the list\n",
      "________________________________________________________________________________\n",
      "Pclass\n",
      "1    84.15\n",
      "2    20.66\n",
      "3    13.68\n",
      "Name: Fare, dtype: float64\n",
      "________________________________________________________________________________\n",
      "Sex\n",
      "female    314\n",
      "male      577\n",
      "Name: Survived, dtype: int64\n",
      "________________________________________________________________________________\n",
      "min     0.42\n",
      "max    80.00\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# 1. Load and print first 5 rows — understand what the data is about\n",
    "print(df.head(5))\n",
    "print(\"_\" * 80)\n",
    "print(df.tail(5))\n",
    "df.head()\n",
    "\n",
    "# 2. Check shape — how many passengers, how many columns\n",
    "print(df.shape)\n",
    "print(\"_\" * 80)\n",
    "\n",
    "# 3. Run .info() — check data types\n",
    "df.info()\n",
    "print(\"_\" * 80)\n",
    "\n",
    "# 4. Run .describe() — look at the numbers carefully\n",
    "print(df.describe())\n",
    "print(\"_\" * 80)\n",
    "\n",
    "# 5. Find missing values — which columns and what percentage\n",
    "print(df.isnull().sum())\n",
    "print((df.isnull().sum()/len(df)) * 100)\n",
    "print(\"_\" * 80)\n",
    "\n",
    "# 6. Check duplicates\n",
    "\n",
    "print(df.duplicated().sum())\n",
    "print(df.drop_duplicates())\n",
    "print(\"_\" * 80)\n",
    "\n",
    "# 7. Find unique values in categorical columns \n",
    "#    (Sex, Embarked, Pclass)\n",
    "print(df[\"Sex\"].unique())\n",
    "print(df[\"Embarked\"].unique())\n",
    "print(df[\"Pclass\"].unique())\n",
    "print(\"_\" * 80)\n",
    "\n",
    "# 8. Check outliers in Age and Fare columns using IQR\n",
    "print(\"mean:-\", df[\"Age\"].mean())\n",
    "print(\"median:-\", df[\"Fare\"].median())\n",
    "\n",
    "Q1 = df[\"Age\"].quantile(0.25)\n",
    "Q3 = df[\"Fare\"].quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 - 1.5 * IQR\n",
    "\n",
    "outliers = df[(df[\"Age\"] < lower) | (df[\"Fare\"] > upper)]\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n",
    "print(outliers)\n",
    "print(\"_\" * 80)\n",
    "\n",
    "\n",
    "# 9. Find correlation — which columns relate most to \"Survived\"\n",
    "print(df.corr(numeric_only=True))\n",
    "print(df.corr(numeric_only=True)[\"Survived\"])\n",
    "print(\"_\" * 80)\n",
    "\n",
    "# 10. Answer these questions using your Pandas skills:\n",
    "# a) How many passengers survived vs didn't survive?\n",
    "#    (value_counts on Survived column)\n",
    "surv_pass = df[\"Survived\"].value_counts()\n",
    "print(surv_pass)\n",
    "print(\"_\" * 80)\n",
    "\n",
    "\n",
    "# b) What was the survival rate? (percentage)\n",
    "surv_count = np.sum(df[\"Survived\"] == 1)\n",
    "surv_count_perc = (surv_count / len(df)) * 100\n",
    "print(\"The survival rate is :-\",surv_count_perc)\n",
    "print(\"_\" * 80)\n",
    "\n",
    "\n",
    "# c) What was the average age of passengers?\n",
    "avg_age = df[\"Age\"].mean().round(2)\n",
    "print(\"The average age of passengers :-\",avg_age)\n",
    "print(\"_\" * 80)\n",
    "\n",
    "# d) How many males vs females were on the ship?\n",
    "males_fema = df[\"Sex\"].value_counts()\n",
    "print(males_fema)\n",
    "print(\"_\" * 80)\n",
    "\n",
    "# e) Which passenger class (Pclass) had most passengers?\n",
    "most_pass = df[\"Pclass\"].value_counts().idxmax()\n",
    "print(f\"The {most_pass} has the most Passengers in the list\")\n",
    "print(\"_\" * 80)\n",
    "\n",
    "# f) Find average Fare per Pclass using groupby\n",
    "avg_fare = df.groupby(\"Pclass\")[\"Fare\"].mean().round(2)\n",
    "print(avg_fare)\n",
    "print(\"_\" * 80)\n",
    "\n",
    "# g) Find survival count per Sex using groupby\n",
    "surv_count_sex = df.groupby(\"Sex\")[\"Survived\"].count()\n",
    "print(surv_count_sex)\n",
    "print(\"_\" * 80)\n",
    "df.head()\n",
    "\n",
    "# h) Find the oldest and youngest passenger on the ship\n",
    "old_young_pass = df[\"Age\"].agg([\"min\" , \"max\"])\n",
    "print(old_young_pass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c714f4",
   "metadata": {},
   "source": [
    "# Store Market BIG_DATASET Real World Problem ⭐⭐⭐⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5536dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
      "0       1  CA-2016-152156   11/8/2016  11/11/2016    Second Class    CG-12520   \n",
      "1       2  CA-2016-152156   11/8/2016  11/11/2016    Second Class    CG-12520   \n",
      "2       3  CA-2016-138688   6/12/2016   6/16/2016    Second Class    DV-13045   \n",
      "3       4  US-2015-108966  10/11/2015  10/18/2015  Standard Class    SO-20335   \n",
      "4       5  US-2015-108966  10/11/2015  10/18/2015  Standard Class    SO-20335   \n",
      "\n",
      "     Customer Name    Segment        Country             City  ...  \\\n",
      "0      Claire Gute   Consumer  United States        Henderson  ...   \n",
      "1      Claire Gute   Consumer  United States        Henderson  ...   \n",
      "2  Darrin Van Huff  Corporate  United States      Los Angeles  ...   \n",
      "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
      "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
      "\n",
      "  Postal Code  Region       Product ID         Category Sub-Category  \\\n",
      "0       42420   South  FUR-BO-10001798        Furniture    Bookcases   \n",
      "1       42420   South  FUR-CH-10000454        Furniture       Chairs   \n",
      "2       90036    West  OFF-LA-10000240  Office Supplies       Labels   \n",
      "3       33311   South  FUR-TA-10000577        Furniture       Tables   \n",
      "4       33311   South  OFF-ST-10000760  Office Supplies      Storage   \n",
      "\n",
      "                                        Product Name     Sales  Quantity  \\\n",
      "0                  Bush Somerset Collection Bookcase  261.9600         2   \n",
      "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400         3   \n",
      "2  Self-Adhesive Address Labels for Typewriters b...   14.6200         2   \n",
      "3      Bretford CR4500 Series Slim Rectangular Table  957.5775         5   \n",
      "4                     Eldon Fold 'N Roll Cart System   22.3680         2   \n",
      "\n",
      "   Discount    Profit  \n",
      "0      0.00   41.9136  \n",
      "1      0.00  219.5820  \n",
      "2      0.00    6.8714  \n",
      "3      0.45 -383.0310  \n",
      "4      0.20    2.5164  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "      Row ID        Order ID Order Date  Ship Date       Ship Mode  \\\n",
      "9989    9990  CA-2014-110422  1/21/2014  1/23/2014    Second Class   \n",
      "9990    9991  CA-2017-121258  2/26/2017   3/3/2017  Standard Class   \n",
      "9991    9992  CA-2017-121258  2/26/2017   3/3/2017  Standard Class   \n",
      "9992    9993  CA-2017-121258  2/26/2017   3/3/2017  Standard Class   \n",
      "9993    9994  CA-2017-119914   5/4/2017   5/9/2017    Second Class   \n",
      "\n",
      "     Customer ID     Customer Name   Segment        Country         City  ...  \\\n",
      "9989    TB-21400  Tom Boeckenhauer  Consumer  United States        Miami  ...   \n",
      "9990    DB-13060       Dave Brooks  Consumer  United States   Costa Mesa  ...   \n",
      "9991    DB-13060       Dave Brooks  Consumer  United States   Costa Mesa  ...   \n",
      "9992    DB-13060       Dave Brooks  Consumer  United States   Costa Mesa  ...   \n",
      "9993    CC-12220      Chris Cortes  Consumer  United States  Westminster  ...   \n",
      "\n",
      "     Postal Code  Region       Product ID         Category Sub-Category  \\\n",
      "9989       33180   South  FUR-FU-10001889        Furniture  Furnishings   \n",
      "9990       92627    West  FUR-FU-10000747        Furniture  Furnishings   \n",
      "9991       92627    West  TEC-PH-10003645       Technology       Phones   \n",
      "9992       92627    West  OFF-PA-10004041  Office Supplies        Paper   \n",
      "9993       92683    West  OFF-AP-10002684  Office Supplies   Appliances   \n",
      "\n",
      "                                           Product Name    Sales  Quantity  \\\n",
      "9989                             Ultra Door Pull Handle   25.248         3   \n",
      "9990  Tenex B1-RE Series Chair Mats for Low Pile Car...   91.960         2   \n",
      "9991                              Aastra 57i VoIP phone  258.576         2   \n",
      "9992  It's Hot Message Books with Stickers, 2 3/4\" x 5\"   29.600         4   \n",
      "9993  Acco 7-Outlet Masterpiece Power Center, Wihtou...  243.160         2   \n",
      "\n",
      "      Discount   Profit  \n",
      "9989       0.2   4.1028  \n",
      "9990       0.0  15.6332  \n",
      "9991       0.2  19.3932  \n",
      "9992       0.0  13.3200  \n",
      "9993       0.0  72.9480  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "____________________________________________________________________________________________________\n",
      "(9994, 21)\n",
      "____________________________________________________________________________________________________\n",
      "['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount', 'Profit']\n",
      "____________________________________________________________________________________________________\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9994 entries, 0 to 9993\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Row ID         9994 non-null   int64  \n",
      " 1   Order ID       9994 non-null   object \n",
      " 2   Order Date     9994 non-null   object \n",
      " 3   Ship Date      9994 non-null   object \n",
      " 4   Ship Mode      9994 non-null   object \n",
      " 5   Customer ID    9994 non-null   object \n",
      " 6   Customer Name  9994 non-null   object \n",
      " 7   Segment        9994 non-null   object \n",
      " 8   Country        9994 non-null   object \n",
      " 9   City           9994 non-null   object \n",
      " 10  State          9994 non-null   object \n",
      " 11  Postal Code    9994 non-null   int64  \n",
      " 12  Region         9994 non-null   object \n",
      " 13  Product ID     9994 non-null   object \n",
      " 14  Category       9994 non-null   object \n",
      " 15  Sub-Category   9994 non-null   object \n",
      " 16  Product Name   9994 non-null   object \n",
      " 17  Sales          9994 non-null   float64\n",
      " 18  Quantity       9994 non-null   int64  \n",
      " 19  Discount       9994 non-null   float64\n",
      " 20  Profit         9994 non-null   float64\n",
      "dtypes: float64(3), int64(3), object(15)\n",
      "memory usage: 1.6+ MB\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "            Row ID   Postal Code         Sales     Quantity     Discount  \\\n",
      "count  9994.000000   9994.000000   9994.000000  9994.000000  9994.000000   \n",
      "mean   4997.500000  55190.379428    229.858001     3.789574     0.156203   \n",
      "std    2885.163629  32063.693350    623.245101     2.225110     0.206452   \n",
      "min       1.000000   1040.000000      0.444000     1.000000     0.000000   \n",
      "25%    2499.250000  23223.000000     17.280000     2.000000     0.000000   \n",
      "50%    4997.500000  56430.500000     54.490000     3.000000     0.200000   \n",
      "75%    7495.750000  90008.000000    209.940000     5.000000     0.200000   \n",
      "max    9994.000000  99301.000000  22638.480000    14.000000     0.800000   \n",
      "\n",
      "            Profit  \n",
      "count  9994.000000  \n",
      "mean     28.656896  \n",
      "std     234.260108  \n",
      "min   -6599.978000  \n",
      "25%       1.728750  \n",
      "50%       8.666500  \n",
      "75%      29.364000  \n",
      "max    8399.976000  \n",
      "____________________________________________________________________________________________________\n",
      "Row ID           0\n",
      "Order ID         0\n",
      "Order Date       0\n",
      "Ship Date        0\n",
      "Ship Mode        0\n",
      "Customer ID      0\n",
      "Customer Name    0\n",
      "Segment          0\n",
      "Country          0\n",
      "City             0\n",
      "State            0\n",
      "Postal Code      0\n",
      "Region           0\n",
      "Product ID       0\n",
      "Category         0\n",
      "Sub-Category     0\n",
      "Product Name     0\n",
      "Sales            0\n",
      "Quantity         0\n",
      "Discount         0\n",
      "Profit           0\n",
      "dtype: int64\n",
      "____________________________________________________________________________________________________\n",
      "Loss making orders: 1871\n",
      "____________________________________________________________________________________________________\n",
      "Total loss amount: $ -156131.29\n",
      "____________________________________________________________________________________________________\n",
      "Category\n",
      "Office Supplies    886\n",
      "Furniture          714\n",
      "Technology         271\n",
      "Name: count, dtype: int64\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_id = \"1lbxmLzSrAgbSouBpkVaD4RP3rndxtNxP\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "df = pd.read_csv(url, encoding=\"latin-1\")\n",
    "df\n",
    "\n",
    "# SECTION A — First Look & Exploration\n",
    "\n",
    "# 1.  Print first 5 rows and last 5 rows\n",
    "print(df.head(5))\n",
    "print(df.tail(5))\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 2.  Check shape — how many orders and columns\n",
    "print(df.shape)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 3.  Print all column names\n",
    "print(df.columns.tolist())\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 4.  Run .info() — check data types\n",
    "print(df.info())\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 5.  Run .describe() — observe Sales, Profit, \n",
    "#     Discount carefully\n",
    "print(df.describe())\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 6.  Find missing values — column wise \n",
    "#     count and percentage\n",
    "print(df.isnull().sum())\n",
    "print(\"_\" * 100)\n",
    "\n",
    "\n",
    "\n",
    "# How many orders have negative profit?\n",
    "print(\"Loss making orders:\", np.sum(df[\"Profit\"] < 0))\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# What is the total loss amount?\n",
    "print(\"Total loss amount: $\", df[df[\"Profit\"] < 0][\"Profit\"].sum().round(2))\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# Which category has most losses?\n",
    "print(df[df[\"Profit\"] < 0][\"Category\"].value_counts())\n",
    "print(\"_\" * 100)\n",
    "\n",
    "\n",
    "\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# SECTION B — Data Cleaning\n",
    "\n",
    "# 7.  Check duplicate rows — print count\n",
    "#     and remove them. Print shape before and after.\n",
    "print(df.duplicated().count())\n",
    "print(df.shape)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "\n",
    "# 8.  Check unique values in these 4 columns:\n",
    "#     - Ship Mode\n",
    "#     - Segment\n",
    "#     - Region\n",
    "#     - Category\n",
    "print(df[\"Ship Mode\"].unique())\n",
    "print(df[\"Segment\"].unique())\n",
    "print(df[\"Region\"].unique())\n",
    "print(df[\"Category\"].unique())\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 9.  Convert \"Order Date\" and \"Ship Date\" \n",
    "#     from string to datetime:\n",
    "#     Hint: df[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"])\n",
    "df[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"])\n",
    "df[\"Ship Date\"] = pd.to_datetime(df[\"Ship Date\"])\n",
    "df.info()\n",
    "\n",
    "# 10. After converting, extract year from Order Date\n",
    "#     into a new column called \"Order_Year\":\n",
    "#     Hint: df[\"Order Date\"].dt.year\n",
    "df[\"Order_Year\"] = df[\"Order Date\"].dt.year\n",
    "df\n",
    "\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# SECTION C — Feature Engineering\n",
    "\n",
    "# 11. Add \"Profit_Margin\" column:\n",
    "#     Formula: (Profit / Sales) * 100\n",
    "#     Round to 2 decimals\n",
    "#     This tells what % of each sale becomes profit\n",
    "df[\"Profit_Margin\"] = ((df[\"Profit\"] / df[\"Sales\"]) * 100).round(2)\n",
    "df\n",
    "\n",
    "# 12. Add \"Processing_Days\" column:\n",
    "#     Formula: (Ship Date - Order Date).dt.days\n",
    "#     This tells how many days it took to ship\n",
    "df[\"Processing_Days\"] = (df[\"Ship Date\"] - df[\"Order Date\"]).dt.days\n",
    "df\n",
    "\n",
    "# 13. Add \"Sales_Category\" column using np.select():\n",
    "#     Sales >= 1000 → \"High Value\"\n",
    "#     Sales >= 500  → \"Medium Value\"\n",
    "#     Otherwise     → \"Low Value\"\n",
    "\n",
    "condition = [\n",
    "    df[\"Sales\"] >= 1000,\n",
    "    df[\"Sales\"] >=500,\n",
    "]\n",
    "\n",
    "choice_list = [\"High Value\",\"Medium Value\"]\n",
    "df[\"Sales_Category\"] = np.select(condition , choice_list, default=\"Low Value\")\n",
    "df\n",
    "\n",
    "# 14. Add \"Profitable\" column using np.where():\n",
    "#     Profit > 0 → \"Yes\"\n",
    "#     Otherwise  → \"No\"\n",
    "df[\"Profitable\"] = np.where(df[\"Profit\"] >0 , \"Yes\" , \"No\")\n",
    "df\n",
    "\n",
    "# 15. Add \"Discount_Type\" column using np.select():\n",
    "#     Discount == 0    → \"No Discount\"\n",
    "#     Discount <= 0.2  → \"Low Discount\"\n",
    "#     Discount <= 0.4  → \"Medium Discount\"\n",
    "#     Otherwise        → \"High Discount\"\n",
    "condition_disc = [\n",
    "    df[\"Discount\"] == 0,\n",
    "    df[\"Discount\"] <= 0.2,\n",
    "    df[\"Discount\"] <= 0.4,\n",
    "]\n",
    "\n",
    "choice_list_disc = [\"No Discount\", \"Low Discount\", \"Medium Discount\"]\n",
    "\n",
    "df[\"Discount_Type\"] = np.select(condition_disc , choice_list_disc , default = \"High Discount\")\n",
    "df\n",
    "\n",
    "\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# SECTION D — Filtering & Selection\n",
    "\n",
    "# 16. Use iloc to select rows 5 to 15, \n",
    "#     first 6 columns\n",
    "filter_rows = df.iloc[5:16 , :6]\n",
    "print(filter_rows)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 17. Use loc to get Order ID, Customer Name, \n",
    "#     Sales, Profit of all LOSS making orders\n",
    "#     (Profit < 0)\n",
    "#     How many loss making orders exist?\n",
    "loss_making = df.loc[df[\"Profit\"] < 0 ,[\"Order ID\" , \"Customer Name\", \"Sales\"]]\n",
    "print(loss_making)\n",
    "print(\"_\" * 100)\n",
    "df\n",
    "\n",
    "# 18. Filter all orders from \"Technology\" category\n",
    "filter_orders_tech = df.loc[df[\"Category\"] == \"Technology\"]\n",
    "# print(df[\"Category\"].unique())\n",
    "print(filter_orders_tech)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 19. Filter orders where:\n",
    "#     Sales > 500 AND Profit > 0\n",
    "filter_orders_sales = df.loc[(df[\"Sales\"] > 500) & (df[\"Profit\"] > 0)]\n",
    "print(filter_orders_sales)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 20. Filter orders from \"West\" OR \"East\" region\n",
    "#     Hint: .isin()\n",
    "# region = [\"West\" , \"East\"]\n",
    "filter_west_Region = df.loc[(df[\"Region\"] == \"West\") | (df[\"Region\"] == \"East\")]\n",
    "print(filter_west_Region)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 21. Filter orders where Discount is 0\n",
    "#     (full price orders)\n",
    "filter_orders_disc = df.loc[df[\"Discount\"] == 0]\n",
    "print(filter_orders_disc)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 22. Find top 10 orders by highest Sales\n",
    "#     Hint: sort_values + head(10)\n",
    "filter_top10 = df.sort_values(\"Sales\" , ascending = False).head(10)\n",
    "print(filter_top10)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 23. Find top 10 most loss making orders\n",
    "#     (lowest profit — most negative)\n",
    "#     Hint: sort_values ascending=True + head(10)\n",
    "filter_top10_loss = df.sort_values(\"Profit\" , ascending = True).head(10)\n",
    "print(filter_top10_loss)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# SECTION E — Groupby Analysis\n",
    "\n",
    "# 24. Find total Sales per Category\n",
    "find_total_sales = df.groupby(\"Category\")[\"Sales\"].sum()\n",
    "print(find_total_sales)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 25. Find total Profit per Category\n",
    "find_total_profit = df.groupby(\"Category\")[\"Profit\"].sum()\n",
    "print(find_total_profit)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 26. Find average Discount per Category\n",
    "find_avg_disc = df.groupby(\"Category\")[\"Discount\"].mean()\n",
    "print(find_avg_disc)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 27. Find total Sales per Region\n",
    "find_avg_disc = df.groupby(\"Region\")[\"Sales\"].sum()\n",
    "print(find_avg_disc)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 28. Find average Processing_Days per Ship Mode\n",
    "#     Which shipping mode is fastest?\n",
    "find_avg_ship = df.groupby(\"Ship Mode\")[\"Processing_Days\"].mean().idxmax()\n",
    "print(f\"{find_avg_ship}shipping mode is fastest\")\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 29. Find total Sales per Order_Year\n",
    "#     How has business grown year by year?\n",
    "find_total_sales_orderyear = df.groupby(\"Order_Year\")[\"Sales\"].sum().diff()\n",
    "print(f\"This is the bussiness Sales \\n {find_total_sales_orderyear } values grown year by year\",)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 30. Find mean, max, min of Profit \n",
    "#     per Segment in ONE line using .agg()\n",
    "find_mean_max_min = df[\"Profit\"].agg([\"mean\" , \"max\" , \"min\"])\n",
    "print(find_mean_max_min)\n",
    "print(\"_\" * 100)\n",
    "# 31. Find which Sub-Category has \n",
    "#     highest total Sales\n",
    "#     Hint: groupby + sum + idxmax()\n",
    "find_subcategory = df.groupby(\"Category\")[\"Sales\"].sum().idxmax()\n",
    "print(f\"{find_subcategory} has the highest total sales\")\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 32. Find which Sub-Category has \n",
    "#     highest total Profit\n",
    "find_subcat_profit = df.groupby(\"Category\")[\"Profit\"].sum().idxmax()\n",
    "print(f\"{find_subcat_profit} has the highest total profit\")\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 33. Find which Sub-Category has \n",
    "#     most LOSS (most negative profit)\n",
    "#     Hint: groupby + sum + idxmin()\n",
    "find_subcat_loss_pro = df.groupby(\"Category\")[\"Profit\"].sum().idxmin()\n",
    "print(f\"{find_subcat_loss_pro} has the most negative profit\")\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# SECTION F — Business Insights\n",
    "\n",
    "# 34. What percentage of orders are loss making?\n",
    "#     (Profit < 0)\n",
    "#     Use np.sum() method\n",
    "total_loss_profit = np.sum(df[\"Profit\"] < 0)\n",
    "find_perc_loss = (total_loss_profit / len(df))* 100\n",
    "print(f\"{find_perc_loss.round(2)} percentage of orders are loss making\")\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 35. Find top 5 customers by total Sales\n",
    "#     Hint: groupby \"Customer Name\" + \n",
    "#     sum + sort_values + head(5)\n",
    "top5_cust_sales = df.groupby(\"Customer Name\")[\"Sales\"].sum().sort_values().head(5)\n",
    "print(top5_cust_sales)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 36. Find which State has highest total Sales\n",
    "#     Hint: groupby \"State\" + sum + idxmax()\n",
    "find_state_sales = df.groupby(\"State\")[\"Sales\"].sum().idxmax()\n",
    "print(f\"{find_state_sales} has highest total Sales\")\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 37. Find which Ship Mode is used most\n",
    "#     Hint: value_counts()\n",
    "find_shipmode = df[\"Ship Mode\"].value_counts().idxmax()\n",
    "print(f\"{find_shipmode} Ship Mode is used most\")\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 38. Find average Profit Margin per Category\n",
    "#     Hint: groupby + mean on Profit_Margin column\n",
    "find_avg_pro_margin = df.groupby(\"Category\")[\"Profit_Margin\"].mean()\n",
    "print(find_avg_pro_margin)\n",
    "print(\"_\" * 100)\n",
    "# df\n",
    "\n",
    "\n",
    "\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# SECTION G — Think Like a Data Analyst\n",
    "\n",
    "# 39. Find total Sales AND total Profit together\n",
    "#     for each Category + Region combination\n",
    "#     Hint: groupby([\"Category\",\"Region\"])[[\"Sales\",\"Profit\"]].sum()\n",
    "find_sale_profit_toget = df.groupby([\"Category\" , \"Region\"])[[\"Sales\" , \"Profit\"]].sum()\n",
    "print(find_sale_profit_toget)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 40. Find what % of total Sales \n",
    "#     comes from each Category:\n",
    "#     Formula:\n",
    "#     cat_sales = df.groupby(\"Category\")[\"Sales\"].sum()\n",
    "#     total_sales = df[\"Sales\"].sum()\n",
    "#     percentage = (cat_sales / total_sales) * 100\n",
    "#     print(percentage.round(2))\n",
    "cat_percenteage_sales = df.groupby(\"Category\")[\"Sales\"].sum()\n",
    "total_sales = df[\"Sales\"].sum()\n",
    "percentage = (cat_percenteage_sales / total_sales) * 100\n",
    "print(percentage.round(2))\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 41. Find the most profitable Region\n",
    "#     (highest total Profit)\n",
    "find_profitable_region = df.groupby(\"Region\")[\"Profit\"].sum().idxmax()\n",
    "print(f\"{find_profitable_region} is the most profitable Region\")\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 42. Find the year with highest total Sales\n",
    "#     Hint: groupby \"Order_Year\" + sum + idxmax()\n",
    "find_year_max_sales = df.groupby(\"Order_Year\")[\"Sales\"].sum().idxmax()\n",
    "print(f\"{find_year_max_sales} is the year with highest total Sales\")\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 43. Find which Segment brings most Revenue\n",
    "#     AND most Profit — are they the same?\n",
    "find_seg_max_rev_pro = df.groupby(\"Segment\")[\"Profit\"].sum().idxmax()\n",
    "print(find_seg_max_rev_pro)\n",
    "print(\"_\" * 100)\n",
    "\n",
    "# 44. Find top 3 States by total Profit\n",
    "find_top3_states = df.groupby(\"State\")[\"Profit\"].sum().sort_values(ascending=False).head(3)\n",
    "print(find_top3_states)\n",
    "\n",
    "# 45. Write 3 business insights in comments\n",
    "#     based on everything you found:\n",
    "\n",
    "#     # Insight 1: ___ This data is all about the supermarket store data of all branches in the cities and states etc everything about profits and losses etc \n",
    "#     # Insight 2: ___ there are so many  losses due heavy offers conducting by the company which is not relevant\n",
    "#     # Insight 3: ___california , new york , washington are the top 3 companies to get high profits and suppose if company want the high profits , do not keep high offers\n",
    "                        # check the non selling stock and put them in offer in festival days and some other related days so , the stock will be over and company \n",
    "                        # will get more profits and it will grow\n",
    "    \n",
    "#     Think like a business analyst —\n",
    "#     what does this data tell the company?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b43052",
   "metadata": {},
   "source": [
    "# # DIAMOND ANALYSIS⭐✅⭐ - Real_World Problem Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506cc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   carat      cut color clarity  depth  table  price     x     y     z\n",
      "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
      "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
      "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
      "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
      "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n",
      "       carat        cut color clarity  depth  table  price     x     y     z\n",
      "53935   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75  5.76  3.50\n",
      "53936   0.72       Good     D     SI1   63.1   55.0   2757  5.69  5.75  3.61\n",
      "53937   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66  5.68  3.56\n",
      "53938   0.86    Premium     H     SI2   61.0   58.0   2757  6.15  6.12  3.74\n",
      "53939   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83  5.87  3.64\n",
      "____________________________________________________________________________________________________\n",
      "(53940, 10)\n",
      "____________________________________________________________________________________________________\n",
      "['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'price', 'x', 'y', 'z']\n",
      "____________________________________________________________________________________________________\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53940 entries, 0 to 53939\n",
      "Data columns (total 10 columns):\n",
      " #   Column   Non-Null Count  Dtype   \n",
      "---  ------   --------------  -----   \n",
      " 0   carat    53940 non-null  float64 \n",
      " 1   cut      53940 non-null  category\n",
      " 2   color    53940 non-null  category\n",
      " 3   clarity  53940 non-null  category\n",
      " 4   depth    53940 non-null  float64 \n",
      " 5   table    53940 non-null  float64 \n",
      " 6   price    53940 non-null  int64   \n",
      " 7   x        53940 non-null  float64 \n",
      " 8   y        53940 non-null  float64 \n",
      " 9   z        53940 non-null  float64 \n",
      "dtypes: category(3), float64(6), int64(1)\n",
      "memory usage: 3.0 MB\n",
      "0.0 percentage of missing values\n",
      "____________________________________________________________________________________________________\n",
      "['Ideal', 'Premium', 'Good', 'Very Good', 'Fair']\n",
      "Categories (5, object): ['Ideal', 'Premium', 'Very Good', 'Good', 'Fair']\n",
      "['E', 'I', 'J', 'H', 'F', 'G', 'D']\n",
      "Categories (7, object): ['D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
      "['SI2', 'SI1', 'VS1', 'VS2', 'VVS2', 'VVS1', 'I1', 'IF']\n",
      "Categories (8, object): ['IF', 'VVS1', 'VVS2', 'VS1', 'VS2', 'SI1', 'SI2', 'I1']\n",
      "cut        5\n",
      "color      7\n",
      "clarity    8\n",
      "dtype: int64\n",
      "____________________________________________________________________________________________________\n",
      "The most common cut is Ideal\n",
      "____________________________________________________________________________________________________\n",
      "The shape of dataset Before removing Duplicates :- (53940, 10)\n",
      "The shape of dataset after removing Duplicates :- (53794, 10)\n",
      "____________________________________________________________________________________________________\n",
      "x     7\n",
      "y     6\n",
      "z    19\n",
      "dtype: int64\n",
      "____________________________________________________________________________________________________\n",
      "The shape of dataset Before removing Duplicates :- (53794, 10)\n",
      "The shape of dataset after removing Duplicates :- (53775, 10)\n",
      "____________________________________________________________________________________________________\n",
      "1\n",
      "(53775, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:84: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-aec15220-b9ba-49d3-853b-4256d4c1bc98\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>price_per_carat</th>\n",
       "      <th>volume</th>\n",
       "      <th>price_category</th>\n",
       "      <th>carat_category</th>\n",
       "      <th>cut_score</th>\n",
       "      <th>depth_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>1417.39</td>\n",
       "      <td>38.20</td>\n",
       "      <td>Budget</td>\n",
       "      <td>Tiny</td>\n",
       "      <td>5</td>\n",
       "      <td>Non Ideal Depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1552.38</td>\n",
       "      <td>34.51</td>\n",
       "      <td>Budget</td>\n",
       "      <td>Tiny</td>\n",
       "      <td>4</td>\n",
       "      <td>Non Ideal Depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1421.74</td>\n",
       "      <td>38.08</td>\n",
       "      <td>Budget</td>\n",
       "      <td>Tiny</td>\n",
       "      <td>2</td>\n",
       "      <td>Non Ideal Depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "      <td>1151.72</td>\n",
       "      <td>46.72</td>\n",
       "      <td>Budget</td>\n",
       "      <td>Tiny</td>\n",
       "      <td>4</td>\n",
       "      <td>Non Ideal Depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1080.65</td>\n",
       "      <td>51.92</td>\n",
       "      <td>Budget</td>\n",
       "      <td>Tiny</td>\n",
       "      <td>2</td>\n",
       "      <td>Non Ideal Depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53935</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>60.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3829.17</td>\n",
       "      <td>115.92</td>\n",
       "      <td>Luxury</td>\n",
       "      <td>Small</td>\n",
       "      <td>5</td>\n",
       "      <td>Non Ideal Depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53936</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.69</td>\n",
       "      <td>5.75</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3829.17</td>\n",
       "      <td>118.11</td>\n",
       "      <td>Luxury</td>\n",
       "      <td>Small</td>\n",
       "      <td>2</td>\n",
       "      <td>Non Ideal Depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53937</th>\n",
       "      <td>0.70</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.66</td>\n",
       "      <td>5.68</td>\n",
       "      <td>3.56</td>\n",
       "      <td>3938.57</td>\n",
       "      <td>114.45</td>\n",
       "      <td>Luxury</td>\n",
       "      <td>Small</td>\n",
       "      <td>3</td>\n",
       "      <td>Non Ideal Depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.86</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3205.81</td>\n",
       "      <td>140.77</td>\n",
       "      <td>Luxury</td>\n",
       "      <td>Small</td>\n",
       "      <td>4</td>\n",
       "      <td>Non Ideal Depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.75</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3676.00</td>\n",
       "      <td>124.57</td>\n",
       "      <td>Luxury</td>\n",
       "      <td>Small</td>\n",
       "      <td>5</td>\n",
       "      <td>Non Ideal Depth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53775 rows × 16 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aec15220-b9ba-49d3-853b-4256d4c1bc98')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-aec15220-b9ba-49d3-853b-4256d4c1bc98 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-aec15220-b9ba-49d3-853b-4256d4c1bc98');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       carat        cut color clarity  depth  table  price     x     y     z  \\\n",
       "0       0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43   \n",
       "1       0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31   \n",
       "2       0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31   \n",
       "3       0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63   \n",
       "4       0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75   \n",
       "...      ...        ...   ...     ...    ...    ...    ...   ...   ...   ...   \n",
       "53935   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75  5.76  3.50   \n",
       "53936   0.72       Good     D     SI1   63.1   55.0   2757  5.69  5.75  3.61   \n",
       "53937   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66  5.68  3.56   \n",
       "53938   0.86    Premium     H     SI2   61.0   58.0   2757  6.15  6.12  3.74   \n",
       "53939   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83  5.87  3.64   \n",
       "\n",
       "       price_per_carat  volume price_category carat_category  cut_score  \\\n",
       "0              1417.39   38.20         Budget           Tiny          5   \n",
       "1              1552.38   34.51         Budget           Tiny          4   \n",
       "2              1421.74   38.08         Budget           Tiny          2   \n",
       "3              1151.72   46.72         Budget           Tiny          4   \n",
       "4              1080.65   51.92         Budget           Tiny          2   \n",
       "...                ...     ...            ...            ...        ...   \n",
       "53935          3829.17  115.92         Luxury          Small          5   \n",
       "53936          3829.17  118.11         Luxury          Small          2   \n",
       "53937          3938.57  114.45         Luxury          Small          3   \n",
       "53938          3205.81  140.77         Luxury          Small          4   \n",
       "53939          3676.00  124.57         Luxury          Small          5   \n",
       "\n",
       "         depth_quality  \n",
       "0      Non Ideal Depth  \n",
       "1      Non Ideal Depth  \n",
       "2      Non Ideal Depth  \n",
       "3      Non Ideal Depth  \n",
       "4      Non Ideal Depth  \n",
       "...                ...  \n",
       "53935  Non Ideal Depth  \n",
       "53936  Non Ideal Depth  \n",
       "53937  Non Ideal Depth  \n",
       "53938  Non Ideal Depth  \n",
       "53939  Non Ideal Depth  \n",
       "\n",
       "[53775 rows x 16 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "df\n",
    "\n",
    "# print(df.columns.tolist())\n",
    "# print(df.head(2))\n",
    "# print(df.shape)\n",
    "\n",
    "# SECTION A — First Look & Exploration\n",
    "\n",
    "# 1.  Print first 5 and last 5 rows\n",
    "print(df.head(5))\n",
    "print(df.tail(5))\n",
    "print(\"_\"*100)\n",
    "\n",
    "# 2.  Check shape — how many diamonds and columns\n",
    "print(df.shape)\n",
    "print(\"_\"*100)\n",
    "\n",
    "# 3.  Print all column names\n",
    "print(df.columns.tolist())\n",
    "print(\"_\"*100)\n",
    "\n",
    "# 4.  Run .info() — check all data types\n",
    "df.info()\n",
    "\n",
    "# 5.  Run .describe() — observe price, carat, \n",
    "#     depth carefully. Note the min and max values.\n",
    "df.describe()\n",
    "\n",
    "# 6.  Find missing values — column wise count\n",
    "#     and percentage\n",
    "\n",
    "find_missing = df.isnull().sum().sum()\n",
    "miss_perc = (find_missing / len(df)) * 100\n",
    "print(f\"{miss_perc} percentage of missing values\")\n",
    "print(\"_\"*100)\n",
    "\n",
    "# 7.  Check unique values in these 3 columns:\n",
    "#     - cut\n",
    "#     - color  \n",
    "#     - clarity\n",
    "print(df[\"cut\"].unique())\n",
    "print(df[\"color\"].unique())\n",
    "print(df[\"clarity\"].unique())\n",
    "#  or\n",
    "print(df[[\"cut\" , \"color\" , \"clarity\"]].nunique())\n",
    "print(\"_\"*100)\n",
    "\n",
    "# 8.  Check value distribution of cut column\n",
    "#     using value_counts()\n",
    "#     What is the most common cut?\n",
    "most_cut = df[\"cut\"].value_counts().idxmax()\n",
    "print(f\"The most common cut is {most_cut}\")\n",
    "print(\"_\"*100)\n",
    "# -----------------------------------------------\n",
    "\n",
    "\n",
    "# SECTION B — Data Cleaning & Validation\n",
    "\n",
    "# 9.  Check for duplicate rows — count and remove.\n",
    "#     Print shape before and after.\n",
    "df.duplicated().sum()\n",
    "print(\"The shape of dataset Before removing Duplicates :-\",df.shape)\n",
    "df.drop_duplicates(inplace = True)\n",
    "print(\"The shape of dataset after removing Duplicates :-\",df.shape)\n",
    "print(\"_\"*100)\n",
    "\n",
    "# 10. Check if x, y, z columns have any \n",
    "#     zero values — these are invalid \n",
    "#     (a diamond can't have 0 dimensions):\n",
    "#     Hint: np.sum(df[\"x\"] == 0)\n",
    "#     Check all three: x, y, z\n",
    "checking_cols_0 = np.sum(df[[\"x\",\"y\",\"z\"]] == 0)\n",
    "print(checking_cols_0)\n",
    "print(\"_\"*100)\n",
    "\n",
    "# 11. Remove rows where x == 0 OR y == 0 OR z == 0\n",
    "#     Print shape before and after removing\n",
    "print(\"The shape of dataset Before removing Duplicates :-\",df.shape)\n",
    "df = df[(df[\"x\"] != 0) & (df[\"y\"]!=0) & (df[\"z\"] !=0)]\n",
    "print(\"The shape of dataset after removing Duplicates :-\",df.shape)\n",
    "print(\"_\"*100)\n",
    "\n",
    "# 12. Check if price has any values below 0\n",
    "#     (negative price is impossible)\n",
    "#     Hint: np.sum(df[\"price\"] <= 0)\n",
    "np.sum(df[\"price\"] <= 0)\n",
    "\n",
    "# 13. Check if carat has any values above 5\n",
    "#     (extremely rare — possible data error)\n",
    "#     Hint: np.sum(df[\"carat\"] > 5)\n",
    "checking_carat = (df[\"carat\"] > 5).sum()\n",
    "print(checking_carat)\n",
    "\n",
    "# 14. After all cleaning verify final shape\n",
    "print(df.shape)\n",
    "\n",
    "# -----------------------------------------------\n",
    "\n",
    "# SECTION C — Feature Engineering\n",
    "\n",
    "# 15. Add \"price_per_carat\" column:\n",
    "#     Formula: price / carat  (round to 2)\n",
    "#     This tells the value per unit weight\n",
    "df[\"price_per_carat\"] = (df[\"price\"] / df[\"carat\"]).round(2)\n",
    "\n",
    "# 16. Add \"volume\" column:\n",
    "#     Formula: x * y * z  (round to 2)\n",
    "#     This is approximate diamond volume in mm³\n",
    "df[\"volume\"] = (df['x'] * df['y'] * df['z']).round(2)\n",
    "df\n",
    "\n",
    "# 17. Add \"price_category\" column using np.select():\n",
    "#     price >= 10000 → \"Luxury\"\n",
    "#     price >= 5000  → \"Premium\"\n",
    "#     price >= 2000  → \"Mid Range\"\n",
    "#     Otherwise      → \"Budget\"\n",
    "condition =[\n",
    "    df[\"price\"] >= 1000,\n",
    "    df[\"price\"] >= 5000,\n",
    "    df[\"price\"] >= 2000\n",
    "]\n",
    "\n",
    "choice_list = [\"Luxury\" , \"Premium\", \"Mid Range\"]\n",
    "df[\"price_category\"]  = np.select(condition , choice_list, default=\"Budget\")\n",
    "\n",
    "# 18. Add \"carat_category\" column using np.select():\n",
    "#     carat >= 2.0 → \"Large\"\n",
    "#     carat >= 1.0 → \"Medium\"\n",
    "#     carat >= 0.5 → \"Small\"\n",
    "#     Otherwise    → \"Tiny\"\n",
    "condition_carat = [\n",
    "    df[\"carat\"]>=2.0,\n",
    "    df[\"carat\"]>=1.0,\n",
    "    df[\"carat\"]>=0.5\n",
    "]\n",
    "\n",
    "choice_list_carat = [\"Large\" , \"Medium\", \"Small\" ]\n",
    "df[\"carat_category\"] = np.select(condition_carat , choice_list_carat , default=\"Tiny\")\n",
    "df\n",
    "\n",
    "# 19. Add \"cut_score\" column using np.select():\n",
    "#     cut == \"Ideal\"     → 5\n",
    "#     cut == \"Premium\"   → 4\n",
    "#     cut == \"Very Good\" → 3\n",
    "#     cut == \"Good\"      → 2\n",
    "#     Otherwise          → 1\n",
    "#     This converts text category to number\n",
    "condition_cut_score = [\n",
    "    df[\"cut\"] == \"Ideal\",\n",
    "    df[\"cut\"] == \"Premium\",\n",
    "    df[\"cut\"] == \"Very Good\",\n",
    "    df[\"cut\"] == \"Good\"\n",
    "]\n",
    "\n",
    "choice_list_cut_score = [5 , 4,3,2]\n",
    "df[\"cut_score\"] = np.select(condition_cut_score , choice_list_cut_score , default = 1)\n",
    "df\n",
    "\n",
    "# 20. Add \"depth_quality\" column using np.where():\n",
    "#     depth between 60 and 65 → \"Ideal Depth\"\n",
    "#     Otherwise               → \"Non Ideal Depth\"\n",
    "#     Hint: \n",
    "#     np.where((df[\"depth\"]>=60) & (df[\"depth\"]<=65),\n",
    "#     \"Ideal Depth\", \"Non Ideal Depth\")\n",
    "\n",
    "df[\"depth_quality\"] = np.where((df[\"depth\"]>60) & (df[\"depth\"] > 65) , \"Ideal Depth\", \"Non Ideal Depth\")\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff27c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
